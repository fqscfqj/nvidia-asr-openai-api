# -*- coding: utf-8 -*-
"""
FastAPI ä¸»åº”ç”¨æ¨¡å—

æä¾› OpenAI Whisper å…¼å®¹çš„ REST API æ¥å£:
- POST /v1/audio/transcriptions - éŸ³é¢‘è½¬å½•
- GET /health - å¥åº·æ£€æŸ¥
- GET /status - æ¨¡å‹çŠ¶æ€
- POST /model/load - é¢„åŠ è½½æ¨¡å‹
- POST /model/unload - å¸è½½æ¨¡å‹
"""

import os
import sys
from typing import Optional, List
from contextlib import asynccontextmanager

from fastapi import FastAPI, File, UploadFile, Form, HTTPException, Query, Header, Depends
from fastapi.responses import PlainTextResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field
from loguru import logger

from .multi_model_manager import get_multi_model_manager, shutdown_multi_model_manager
from .model_manager import get_model_manager, shutdown_model_manager
from .engine import get_transcription_engine


# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================

def setup_logging():
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    ä½¿ç”¨ loguru åº“å®ç°ç»“æ„åŒ–æ—¥å¿—, æ”¯æŒ:
    - æ§åˆ¶å°å½©è‰²è¾“å‡º
    - æ–‡ä»¶æ—¥å¿—è½®è½¬
    - æ—¥å¿—çº§åˆ«é…ç½®
    """
    log_level = os.getenv("LOG_LEVEL", "INFO").upper()
    
    # ç§»é™¤é»˜è®¤å¤„ç†å™¨
    logger.remove()
    
    # æ·»åŠ æ§åˆ¶å°å¤„ç†å™¨
    logger.add(
        sys.stdout,
        level=log_level,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | "
               "<level>{level: <8}</level> | "
               "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> | "
               "<level>{message}</level>",
        colorize=True,
    )
    
    logger.info(f"æ—¥å¿—ç³»ç»Ÿå·²åˆå§‹åŒ–, çº§åˆ«: {log_level}")


# ============================================================================
# API Key éªŒè¯
# ============================================================================

security = HTTPBearer(auto_error=False)

def verify_api_key(credentials: Optional[HTTPAuthorizationCredentials] = Depends(security)):
    """
    éªŒè¯ API Key
    
    å¦‚æœè®¾ç½®äº† API_KEY ç¯å¢ƒå˜é‡, åˆ™éœ€è¦åœ¨è¯·æ±‚å¤´ä¸­æä¾›æœ‰æ•ˆçš„ API Key:
    Authorization: Bearer <API_KEY>
    
    å¦‚æœæœªè®¾ç½® API_KEY, åˆ™ä¸è¿›è¡ŒéªŒè¯
    """
    api_key = os.getenv("API_KEY", "")
    
    # å¦‚æœæœªé…ç½® API_KEY, åˆ™ä¸è¿›è¡ŒéªŒè¯
    if not api_key:
        return None
    
    # æ£€æŸ¥æ˜¯å¦æä¾›äº†å‡­è¯
    if not credentials:
        raise HTTPException(
            status_code=401,
            detail="æœªæä¾› API Key",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    # éªŒè¯ API Key
    if credentials.credentials != api_key:
        raise HTTPException(
            status_code=401,
            detail="æ— æ•ˆçš„ API Key",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    return credentials.credentials


# ============================================================================
# åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
# ============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
    
    å¯åŠ¨æ—¶:
    - åˆå§‹åŒ–æ—¥å¿—
    - åˆ›å»ºå¤šæ¨¡å‹ç®¡ç†å™¨ (ä½†ä¸åŠ è½½æ¨¡å‹, å®ç°æ‡’åŠ è½½)
    
    å…³é—­æ—¶:
    - å¸è½½æ¨¡å‹
    - é‡Šæ”¾ GPU èµ„æº
    """
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    setup_logging()
    logger.info("=== NVIDIA ASR to OpenAI API æœåŠ¡å¯åŠ¨ ===")
    default_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "models")
    logger.info(f"æ¨¡å‹è·¯å¾„: {os.getenv('MODEL_PATH', default_path)}")
    logger.info(f"è¶…æ—¶æ—¶é—´: {os.getenv('MODEL_TIMEOUT_SEC', '300')}ç§’")
    logger.info(f"FP16 æ¨¡å¼: {os.getenv('USE_FP16', 'true')}")
    logger.info(f"å¯ç”¨æ¨¡å‹: {os.getenv('ENABLED_MODELS', 'canary-1b-v2')}")
    
    # API Key é…ç½®
    if os.getenv("API_KEY"):
        logger.info("API Key éªŒè¯: å·²å¯ç”¨")
    else:
        logger.warning("API Key éªŒè¯: æœªå¯ç”¨ (å»ºè®®è®¾ç½® API_KEY ç¯å¢ƒå˜é‡)")
    
    # åˆå§‹åŒ–å¤šæ¨¡å‹ç®¡ç†å™¨ (ä»…åˆ›å»ºå®ä¾‹, ä¸åŠ è½½æ¨¡å‹)
    _ = get_multi_model_manager()
    logger.info("å¤šæ¨¡å‹ç®¡ç†å™¨å·²å°±ç»ª (æ‡’åŠ è½½æ¨¡å¼, é¦–æ¬¡è¯·æ±‚æ—¶åŠ è½½å¯¹åº”æ¨¡å‹)")
    
    yield
    
    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("=== æ­£åœ¨å…³é—­ API æœåŠ¡ ===")
    shutdown_multi_model_manager()
    logger.info("API æœåŠ¡å·²å…³é—­")


# ============================================================================
# FastAPI åº”ç”¨å®ä¾‹
# ============================================================================

app = FastAPI(
    title="NVIDIA ASR to OpenAI API",
    version="1.0.0",
    description="""
## NVIDIA ASR to OpenAI API

å…¼å®¹ OpenAI Whisper API çš„è¯­éŸ³è½¬å½•æœåŠ¡ï¼Œæ”¯æŒ canary-1b-v2 å’Œ parakeet-tdt-0.6b-v3 æ¨¡å‹ã€‚

### ç‰¹æ€§
- ğŸš€ **æ‡’åŠ è½½**: é¦–æ¬¡è¯·æ±‚æ—¶æ‰åŠ è½½æ¨¡å‹, èŠ‚çœèµ„æº
- â±ï¸ **è‡ªåŠ¨å¸è½½**: é—²ç½®è¶…æ—¶åè‡ªåŠ¨é‡Šæ”¾ GPU æ˜¾å­˜
- ğŸ¯ **é«˜ç²¾åº¦**: æ”¯æŒ 25 ç§æ¬§æ´²è¯­è¨€çš„è½¬å½•å’Œç¿»è¯‘
- ğŸ“ **å¤šæ ¼å¼**: æ”¯æŒ text/json/srt/vtt/verbose_json è¾“å‡ºæ ¼å¼

### æ”¯æŒçš„è¯­è¨€
en, de, fr, es, it, pt, nl, pl, ru, uk, cs, sk, bg, hr, da, fi, el, hu, ro, sv, et, lv, lt, sl, mt
    """,
    lifespan=lifespan,
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
# æ•°æ®æ¨¡å‹
# ============================================================================

class TranscriptionResponse(BaseModel):
    """è½¬å½•å“åº”æ¨¡å‹ (JSON æ ¼å¼)"""
    text: str
    language: Optional[str] = None
    duration: Optional[float] = None


class VerboseTranscriptionResponse(BaseModel):
    """è¯¦ç»†è½¬å½•å“åº”æ¨¡å‹ (verbose_json æ ¼å¼)"""
    task: str = "transcribe"
    text: str
    language: Optional[str] = None
    duration: Optional[float] = None
    segments: list = []
    words: Optional[list] = None


class ModelStatusResponse(BaseModel):
    """æ¨¡å‹çŠ¶æ€å“åº”"""
    model_loaded: bool
    model_name: str
    model_path: str
    usage_count: int
    idle_seconds: Optional[float] = None
    timeout_seconds: int
    use_fp16: bool
    gpu_available: bool
    gpu_name: Optional[str] = None
    gpu_memory_allocated_mb: Optional[float] = None
    gpu_memory_reserved_mb: Optional[float] = None


class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    message: str


class OperationResponse(BaseModel):
    """æ“ä½œå“åº”"""
    success: bool
    message: str


class ModelInfo(BaseModel):
    """æ¨¡å‹ä¿¡æ¯"""
    id: str = Field(..., description="æ¨¡å‹ID")
    object: str = Field(default="model", description="å¯¹è±¡ç±»å‹")
    created: int = Field(default=1699000000, description="åˆ›å»ºæ—¶é—´æˆ³")
    owned_by: str = Field(default="nvidia", description="æ‹¥æœ‰è€…")


class ModelListResponse(BaseModel):
    """æ¨¡å‹åˆ—è¡¨å“åº”"""
    object: str = Field(default="list", description="å¯¹è±¡ç±»å‹")
    data: List[ModelInfo] = Field(..., description="æ¨¡å‹åˆ—è¡¨")


# ============================================================================
# API è·¯ç”±
# ============================================================================

@app.get("/", response_class=PlainTextResponse)
async def root():
    """
    æ ¹è·¯ç”± - è¿”å›æœåŠ¡ä¿¡æ¯
    """
    return "NVIDIA ASR to OpenAI API - å…¼å®¹ OpenAI Whisper API çš„è¯­éŸ³è¯†åˆ«æœåŠ¡"


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """
    å¥åº·æ£€æŸ¥ç«¯ç‚¹
    
    ç”¨äº Docker å¥åº·æ£€æŸ¥å’Œè´Ÿè½½å‡è¡¡å™¨æ¢æµ‹
    """
    return HealthResponse(
        status="healthy",
        message="æœåŠ¡è¿è¡Œæ­£å¸¸"
    )


@app.get("/v1/models", response_model=ModelListResponse)
async def list_models(api_key: Optional[str] = Depends(verify_api_key)):
    """
    è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
    
    å…¼å®¹ OpenAI API çš„ /v1/models ç«¯ç‚¹
    è¿”å›å½“å‰å¯ç”¨çš„æ‰€æœ‰æ¨¡å‹
    """
    multi_manager = get_multi_model_manager()
    enabled_models = multi_manager.get_enabled_models()
    
    models_data = [
        ModelInfo(
            id=model_name,
            object="model",
            created=1699000000,
            owned_by="nvidia"
        )
        for model_name in enabled_models
    ]
    
    return ModelListResponse(
        object="list",
        data=models_data
    )


@app.get("/status", response_model=ModelStatusResponse)
async def get_status():
    """
    è·å–æ¨¡å‹çŠ¶æ€
    
    è¿”å›æ¨¡å‹åŠ è½½çŠ¶æ€ã€GPU ä½¿ç”¨æƒ…å†µç­‰ä¿¡æ¯
    """
    manager = get_model_manager()
    status = manager.get_status()
    return ModelStatusResponse(**status)


@app.post("/model/load", response_model=OperationResponse)
async def load_model():
    """
    é¢„åŠ è½½æ¨¡å‹
    
    æ‰‹åŠ¨è§¦å‘æ¨¡å‹åŠ è½½, ç”¨äºé¢„çƒ­æœåŠ¡
    """
    try:
        manager = get_model_manager()
        success = manager.force_load()
        
        if success:
            return OperationResponse(
                success=True,
                message="æ¨¡å‹åŠ è½½æˆåŠŸ"
            )
        else:
            return OperationResponse(
                success=False,
                message="æ¨¡å‹åŠ è½½å¤±è´¥"
            )
    except Exception as e:
        logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")


@app.post("/model/unload", response_model=OperationResponse)
async def unload_model():
    """
    å¸è½½æ¨¡å‹
    
    æ‰‹åŠ¨é‡Šæ”¾ GPU æ˜¾å­˜
    """
    try:
        manager = get_model_manager()
        success = manager.force_unload()
        
        if success:
            return OperationResponse(
                success=True,
                message="æ¨¡å‹å·²å¸è½½, æ˜¾å­˜å·²é‡Šæ”¾"
            )
        else:
            return OperationResponse(
                success=False,
                message="æ— æ³•å¸è½½æ¨¡å‹ (å¯èƒ½æœ‰è¯·æ±‚æ­£åœ¨å¤„ç†)"
            )
    except Exception as e:
        logger.error(f"å¸è½½æ¨¡å‹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"å¸è½½æ¨¡å‹å¤±è´¥: {e}")


@app.post("/v1/audio/transcriptions")
async def create_transcription(
    file: UploadFile = File(..., description="è¦è½¬å½•çš„éŸ³é¢‘æ–‡ä»¶"),
    model: str = Form(default="canary-1b-v2", description="æ¨¡å‹åç§°: canary-1b-v2 æˆ– parakeet-tdt-0.6b-v3"),
    language: Optional[str] = Form(default=None, description="éŸ³é¢‘è¯­è¨€ä»£ç , å¦‚ 'en', 'zh'"),
    response_format: str = Form(default="json", description="å“åº”æ ¼å¼: text, json, srt, vtt, verbose_json"),
    temperature: Optional[float] = Form(default=None, description="é‡‡æ ·æ¸©åº¦ (å…¼å®¹å‚æ•°, æš‚ä¸ä½¿ç”¨)"),
    timestamp_granularities: Optional[str] = Form(default=None, description="æ—¶é—´æˆ³ç²’åº¦ (å…¼å®¹å‚æ•°)"),
    api_key: Optional[str] = Depends(verify_api_key),
):
    """
    éŸ³é¢‘è½¬å½• API
    
    å…¼å®¹ OpenAI Whisper API çš„ /v1/audio/transcriptions ç«¯ç‚¹
    
    ## è¯·æ±‚å‚æ•°
    
    - **file**: éŸ³é¢‘æ–‡ä»¶ (æ”¯æŒ wav, flac, mp3, m4a ç­‰æ ¼å¼)
    - **model**: æ¨¡å‹åç§°: canary-1b-v2 æˆ– parakeet-tdt-0.6b-v3
    - **language**: éŸ³é¢‘è¯­è¨€ä»£ç , å¦‚ 'en', 'de', 'fr' ç­‰
    - **response_format**: å“åº”æ ¼å¼
        - `text`: çº¯æ–‡æœ¬
        - `json`: JSON æ ¼å¼ (é»˜è®¤)
        - `srt`: SRT å­—å¹•æ ¼å¼
        - `vtt`: WebVTT å­—å¹•æ ¼å¼
        - `verbose_json`: è¯¦ç»† JSON (åŒ…å«æ—¶é—´æˆ³)
    
    ## å“åº”
    
    æ ¹æ® response_format è¿”å›ä¸åŒæ ¼å¼çš„è½¬å½•ç»“æœ
    
    ## ç¤ºä¾‹
    
    ```python
    import requests
    
    url = "http://localhost:8909/v1/audio/transcriptions"
    files = {"file": open("audio.wav", "rb")}
    data = {"language": "en", "response_format": "json"}
    
    response = requests.post(url, files=files, data=data)
    print(response.json())
    ```
    """
    # éªŒè¯æ¨¡å‹åç§°
    multi_manager = get_multi_model_manager()
    enabled_models = multi_manager.get_enabled_models()
    if model not in enabled_models:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}, å½“å‰å¯ç”¨çš„æ¨¡å‹: {enabled_models}"
        )
    
    # éªŒè¯å“åº”æ ¼å¼
    valid_formats = {"text", "json", "srt", "vtt", "verbose_json"}
    if response_format not in valid_formats:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„å“åº”æ ¼å¼: {response_format}, æ”¯æŒ: {valid_formats}"
        )
    
    # éªŒè¯æ–‡ä»¶ç±»å‹
    if file.content_type:
        allowed_types = {
            "audio/wav", "audio/wave", "audio/x-wav",
            "audio/flac", "audio/x-flac",
            "audio/mpeg", "audio/mp3",
            "audio/mp4", "audio/m4a", "audio/x-m4a",
            "audio/ogg", "audio/webm",
            "application/octet-stream",  # å…è®¸æœªçŸ¥ç±»å‹
        }
        # æ”¾å®½ç±»å‹æ£€æŸ¥, å…è®¸æ›´å¤šæ ¼å¼
        logger.debug(f"æ–‡ä»¶ç±»å‹: {file.content_type}")
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        audio_bytes = await file.read()
        
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©º")
        
        logger.info(
            f"æ”¶åˆ°è½¬å½•è¯·æ±‚ - æ–‡ä»¶: {file.filename}, "
            f"æ¨¡å‹: {model}, "
            f"å¤§å°: {len(audio_bytes)} bytes, "
            f"è¯­è¨€: {language}, æ ¼å¼: {response_format}"
        )
        
        # è·å–è½¬å½•å¼•æ“å¹¶æ‰§è¡Œè½¬å½•
        engine = get_transcription_engine(model_name=model)
        result = engine.transcribe_bytes(
            audio_bytes=audio_bytes,
            filename=file.filename or "audio.wav",
            language=language,
            response_format=response_format,
            timestamps=True,
        )
        
        # æ ¹æ®æ ¼å¼è¿”å›å“åº”
        if response_format in {"text", "srt", "vtt"}:
            return PlainTextResponse(content=result, media_type="text/plain")
        else:
            return JSONResponse(content=result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è½¬å½•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è½¬å½•å¤±è´¥: {e}")


@app.post("/v1/audio/translations")
async def create_translation(
    file: UploadFile = File(..., description="è¦ç¿»è¯‘çš„éŸ³é¢‘æ–‡ä»¶"),
    model: str = Form(default="canary-1b-v2", description="æ¨¡å‹åç§°"),
    response_format: str = Form(default="json", description="å“åº”æ ¼å¼"),
    temperature: Optional[float] = Form(default=None, description="é‡‡æ ·æ¸©åº¦ (å…¼å®¹å‚æ•°)"),
    api_key: Optional[str] = Depends(verify_api_key),
):
    """
    éŸ³é¢‘ç¿»è¯‘ API (ç¿»è¯‘ä¸ºè‹±è¯­)
    
    å…¼å®¹ OpenAI Whisper API çš„ /v1/audio/translations ç«¯ç‚¹
    å°†ä»»æ„æ”¯æŒçš„è¯­è¨€ç¿»è¯‘ä¸ºè‹±è¯­
    
    ## è¯·æ±‚å‚æ•°
    
    - **file**: éŸ³é¢‘æ–‡ä»¶
    - **model**: æ¨¡å‹åç§° (å…¼å®¹å‚æ•°)
    - **response_format**: å“åº”æ ¼å¼ (text, json, srt, vtt, verbose_json)
    
    ## å“åº”
    
    ç¿»è¯‘åçš„è‹±è¯­æ–‡æœ¬
    """
    valid_formats = {"text", "json", "srt", "vtt", "verbose_json"}
    if response_format not in valid_formats:
        raise HTTPException(
            status_code=400,
            detail=f"ä¸æ”¯æŒçš„å“åº”æ ¼å¼: {response_format}"
        )
    
    try:
        audio_bytes = await file.read()
        
        if len(audio_bytes) == 0:
            raise HTTPException(status_code=400, detail="ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©º")
        
        logger.info(
            f"æ”¶åˆ°ç¿»è¯‘è¯·æ±‚ - æ–‡ä»¶: {file.filename}, "
            f"å¤§å°: {len(audio_bytes)} bytes, æ ¼å¼: {response_format}"
        )
        
        # ç¿»è¯‘ä»»åŠ¡: æºè¯­è¨€è®¾ä¸ºè‹±è¯­ (ä¼šè‡ªåŠ¨æ£€æµ‹), ç›®æ ‡è¯­è¨€è®¾ä¸ºè‹±è¯­
        engine = get_transcription_engine(model_name=model)
        result = engine.transcribe_bytes(
            audio_bytes=audio_bytes,
            filename=file.filename or "audio.wav",
            language="en",  # Canary ä¼šè‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
            response_format=response_format,
            timestamps=True,
            target_language="en",  # ç¿»è¯‘åˆ°è‹±è¯­
        )
        
        if response_format in {"text", "srt", "vtt"}:
            return PlainTextResponse(content=result, media_type="text/plain")
        else:
            return JSONResponse(content=result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"ç¿»è¯‘å¤±è´¥: {e}")


# ============================================================================
# ä¸»å…¥å£
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    
    port = int(os.getenv("API_PORT", "8909"))
    
    uvicorn.run(
        "src.main:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        workers=1,  # å• worker, é¿å…å¤šè¿›ç¨‹åŠ è½½æ¨¡å‹
    )
